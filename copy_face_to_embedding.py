from ultralytics import YOLO
from deepface import DeepFace
import cv2
import numpy as np
import faiss
import os
import pickle
import requests
from datetime import datetime

# ===== CONFIG =====
YOLO_WEIGHTS = r"C:\YoLo-Face\runs\detect\train3\weights\best.pt"
DB_PATH = "face_db.index"
LABELS_PATH = "face_labels.pkl"
CONF_THRESH = 0.5
EMBED_MODEL = "ArcFace"   # ArcFace = better accuracy, no TensorFlow dependency
DIST_THRESHOLD = 1.2      # Recommended for ArcFace embeddings
SERVER_URL = "https://noe-uninducible-cheerlessly.ngrok-free.dev"  # change as needed
# ===================

# Load YOLO model
# The YOLO model is used to detect faces (bounding boxes) in each webcam frame.
# We pass the path of weights to create the detection model instance.
model = YOLO(YOLO_WEIGHTS)

# Initialize or load FAISS index
# FAISS holds numeric embeddings for registered faces. `labels` keeps a parallel
# Python list of string IDs/names so we can map an index search result back to
# a human-readable label.
index = None
labels = []
if os.path.exists(DB_PATH) and os.path.exists(LABELS_PATH):
    # If we've previously saved an index and labels, load them so searches
    # can be performed across runs.
    print("[+] Loading existing FAISS index...")
    index = faiss.read_index(DB_PATH)
    with open(LABELS_PATH, "rb") as f:
        labels = pickle.load(f)
else:
    # Otherwise we'll create the FAISS index lazily when the first face is
    # registered (so we know the embedding dimension).
    print("[+] No existing FAISS DB found. It will be created on first registration.")

# Open webcam
# Open the default webcam (device 0). Change the index if you have multiple
# cameras or use a video file path instead.
cap = cv2.VideoCapture(0)
print("[INFO] Press 'r' to register face, 's' to search, 'q' to quit")

# `current_crop` stores the most recently-detected face crop (BGR image).
current_crop = None
# `register_counter` is used to create unique autogenerated labels when the
# user registers a new face with the 'r' key.
register_counter = 0

while True:
    # Read a frame from the webcam
    ret, frame = cap.read()
    if not ret:
        # If the read failed, exit the loop.
        break

    # Run YOLO on the frame to get detections. `results[0].boxes` contains
    # bounding boxes along with confidence scores.
    results = model(frame, verbose=False)
    boxes = results[0].boxes

    # Convert YOLO detections into a simpler list we can use: (x1,y1,x2,y2,area,conf)
    faces = []
    for box in boxes:
        x1, y1, x2, y2 = box.xyxy[0].tolist()
        conf = float(box.conf[0])
        # Skip weak detections below the configured confidence threshold.
        if conf < CONF_THRESH:
            continue
        area = (x2 - x1) * (y2 - y1)
        faces.append((int(x1), int(y1), int(x2), int(y2), area, conf))

    # If multiple faces were found, pick the largest by area (assumed closest/
    # most prominent). We add a small padding before cropping so the face isn't
    # tightly clipped.
    if faces:
        x1, y1, x2, y2, area, conf = max(faces, key=lambda f: f[4])
        pad = 10
        x1p = max(x1 - pad, 0)
        y1p = max(y1 - pad, 0)
        x2p = min(x2 + pad, frame.shape[1])
        y2p = min(y2 + pad, frame.shape[0])
        # Save the cropped face (BGR color as returned by OpenCV)
        current_crop = frame[y1p:y2p, x1p:x2p]

        # Draw a rectangle and confidence on the displayed frame
        cv2.rectangle(frame, (x1p, y1p), (x2p, y2p), (0, 255, 0), 2)
        cv2.putText(frame, f"Face ({conf:.2f})", (x1p, y1p - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

    # Show the live frame
    cv2.imshow("YOLO + ArcFace + FAISS", frame)
    key = cv2.waitKey(1) & 0xFF
    if key == 255:
        # No key pressed; continue the loop
        continue

    # ===== Register new face (auto label) =====
    # ===== Register new face (auto label) =====
        # ===== Register new face (auto label) =====
    if key == ord('r'):
        if current_crop is None:
            print("[!] No face detected to register.")
            continue

        register_counter += 1
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        name = f"face_{register_counter}_{timestamp}"
        print(f"[+] Capturing embedding for {name}...")

        try:
            # Convert to RGB
            rgb = cv2.cvtColor(current_crop, cv2.COLOR_BGR2RGB)

            # Get embedding
            emb = DeepFace.represent(
                rgb,
                model_name=EMBED_MODEL,
                detector_backend="skip",
                enforce_detection=False
            )[0]["embedding"]

            # Convert to numpy + normalize
            emb_np = np.array(emb, dtype="float32").reshape(1, -1)
            emb_np = emb_np / np.linalg.norm(emb_np)

            emb_dim = emb_np.shape[1]
            if index is None:
                print(f"[+] Creating FAISS index with dimension {emb_dim}")
                index = faiss.IndexFlatL2(emb_dim)

            if index.d != emb_dim:
                print("[ERROR] Embedding dimension mismatch. Skipping registration.")
                continue

            # Save locally
            index.add(emb_np)
            labels.append(name)
            faiss.write_index(index, DB_PATH)
            with open(LABELS_PATH, "wb") as f:
                pickle.dump(labels, f)
            print(f"[✓] {name} added to FAISS DB.")

            # ✅ Send to server after saving
            try:
                payload = {
                    "face_index": name,               # string label
                    "embedding": emb                  # <-- raw float list
                }

                resp = requests.post(
                    f"{SERVER_URL}/face-data",
                    json=payload,
                    timeout=10
                )

                if resp.status_code == 200:
                    print(f"[✅] Server commit OK for {name}")
                    print(resp.json())
                else:
                    print(f"[❌] Server error {resp.status_code}: {resp.text}")

            except Exception as e:
                print("[❌] Failed sending to server:", e)

        except Exception as e:
            print("[ERROR] Registration failed:", e)
    # ===== Search for existing face =====
    # ===== Search for existing face =====
    elif key == ord('s'):
        # 's' computes an embedding for the currently-detected face and queries
        # the FAISS index for the nearest neighbor. If the distance is below
        # DIST_THRESHOLD we treat it as a match.
        if current_crop is None:
            print("[!] No face detected to search.")
            continue
        if index is None or len(labels) == 0:
            print("[!] Database is empty.")
            continue

        print("[+] Searching for closest match...")
        try:
            rgb = cv2.cvtColor(current_crop, cv2.COLOR_BGR2RGB)
            emb = DeepFace.represent(
                rgb,
                model_name=EMBED_MODEL,
                detector_backend="skip",
                enforce_detection=False
            )[0]["embedding"]

            emb_np = np.array(emb, dtype="float32").reshape(1, -1)
            emb_np = emb_np / np.linalg.norm(emb_np)  # normalize embedding

            # Search for the single nearest neighbor. `D` contains squared L2
            # distances for IndexFlatL2, and `I` contains the indices.
            D, I = index.search(emb_np, 1)
            name = labels[I[0][0]]
            dist = float(D[0][0])

            # Compare against the configured distance threshold to decide if
            # this is a confident match. Lower threshold = stricter matching.
            if dist < DIST_THRESHOLD:
                print(f"[MATCH] {name} (distance={dist:.4f})")
                cv2.putText(frame, f"{name}", (x1, y1 - 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            else:
                print(f"[NO MATCH] Unknown face (distance={dist:.4f})")
                cv2.putText(frame, "Unknown", (x1, y1 - 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

            # Show the annotated frame briefly so user can see the result.
            cv2.imshow("YOLO + ArcFace + FAISS", frame)
            cv2.waitKey(500)
        except Exception as e:
            print("[ERROR] Search failed:", e)
    
    # ===== Capture face & send embedding to server =====
    elif key == ord('v'):
        if current_crop is None:
            print("[!] No face detected to capture.")
            continue

        print("[+] Capturing embedding & sending to server...")

        try:
            rgb = cv2.cvtColor(current_crop, cv2.COLOR_BGR2RGB)
            emb = DeepFace.represent(
                rgb,
                model_name=EMBED_MODEL,
                detector_backend="skip",
                enforce_detection=False
            )[0]["embedding"]

            emb_np = np.array(emb, dtype="float32").reshape(1, -1)
            emb_np = emb_np / np.linalg.norm(emb_np)

            # Nearest index (optional reference)
            face_index = -1
            if index is not None and len(labels) > 0:
                D, I = index.search(emb_np, 1)
                face_index = int(I[0][0])

            payload = {
                "face_index": face_index,
                "embedding": emb_np.flatten().tolist(),
                "timestamp": datetime.now().isoformat()
            }

            resp = requests.post(SERVER_URL, json=payload)

            if resp.status_code == 200:
                print("[✅] Authentication verified — Biometrics UNLOCKED")
                cv2.putText(frame, "UNLOCKED", (50, 50),
                            cv2.FONT_HERSHEY_SIMPLEX, 1.1, (0, 255, 0), 3)
            else:
                print("[❌] Authentication failed — Access DENIED")
                cv2.putText(frame, "ACCESS DENIED ❌", (50, 50),
                            cv2.FONT_HERSHEY_SIMPLEX, 1.1, (0, 0, 255), 3)

            cv2.imshow("YOLO + ArcFace + FAISS", frame)
            cv2.waitKey(800)

        except Exception as e:
            print("[ERROR] Failed to process:", e)

    elif key == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
